# PiSovereign Configuration
# ===========================
# Complete configuration reference for all system components
#
# Platform Support:
# -----------------
# This configuration works on both:
# - Raspberry Pi 5 + Hailo-10H AI HAT (using hailo-ollama)
# - macOS (Intel/Apple Silicon) (using standard Ollama with Metal)
#
# The inference engine connects to any Ollama-compatible API.
# Model paths for speech processing are auto-detected based on OS.

# ====================
# Environment Settings
# ====================
# Application environment: "development" or "production"
# In production, critical security warnings will block startup unless
# PISOVEREIGN_ALLOW_INSECURE_CONFIG=true is set.
environment = "development"

# ====================
# HTTP Server Settings
# ====================
[server]
# Default to localhost for security (expose explicitly if needed)
host = "127.0.0.1"
port = 3000
cors_enabled = true
# Allowed CORS origins (empty = allow all in dev, CRITICAL warning in production)
allowed_origins = []
# Graceful shutdown timeout in seconds
shutdown_timeout_secs = 30
# Log format: "json" for structured JSON logs, "text" for human-readable
# In production mode, defaults to "json" even if set to "text".
# For development with human-readable logs, keep environment = "development".
log_format = "text"
# Maximum request body size for JSON payloads (default: 1MB = 1048576)
# max_body_size_json_bytes = 1048576
# Maximum request body size for audio uploads (default: 10MB = 10485760)
# max_body_size_audio_bytes = 10485760

# ================================
# AI Inference Engine Settings
# ================================
# Works with any Ollama-compatible backend:
# - Raspberry Pi: Install hailo-ollama from Hailo SDK
# - macOS: Install Ollama via `brew install ollama`
# The API is the same for both platforms.
[inference]
# Base URL of the inference server
# Default: http://localhost:11434 (same for both hailo-ollama and standard Ollama)
base_url = "http://localhost:11434"
# Default model to use for inference
default_model = "qwen2.5:1.5b"
# Request timeout in milliseconds
timeout_ms = 60000
# Maximum tokens to generate
max_tokens = 2048
# Temperature for sampling (0.0 - 2.0)
temperature = 0.7
# Top-p (nucleus) sampling
top_p = 0.9
# System prompt (optional)
# system_prompt = "You are a helpful assistant."

# =====================
# Security Settings
# =====================
[security]
# Whitelisted phone numbers for WhatsApp (empty = allow all)
whitelisted_phones = []

# API Keys (hashed with Argon2id)
# --------------------------------
# Generate hashed keys using: pisovereign-cli hash-api-key <your-key>
# Migrate existing plaintext keys: pisovereign-cli migrate-keys --input config.toml --dry-run
#
# [[security.api_keys]]
# hash = "$argon2id$v=19$m=19456,t=2,p=1$..."
# user_id = "550e8400-e29b-41d4-a716-446655440000"
#
# [[security.api_keys]]
# hash = "$argon2id$v=19$m=19456,t=2,p=1$..."
# user_id = "6ba7b810-9dad-11d1-80b4-00c04fd430c8"

# Trusted reverse proxies (IP addresses)
# Add your proxy IPs here if behind a reverse proxy
# trusted_proxies = ["127.0.0.1", "::1"]

# Enable rate limiting
rate_limit_enabled = true
# Requests per minute per IP
rate_limit_rpm = 60
# Validate TLS certificates for outbound connections
tls_verify_certs = true
# Connection timeout in seconds for external services
connection_timeout_secs = 30
# Minimum TLS version ("1.2" or "1.3")
min_tls_version = "1.2"

# ==============================
# Prompt Security Settings
# ==============================
# Protects against prompt injection and other AI security threats
[prompt_security]
# Enable prompt security analysis
enabled = true
# Sensitivity level: "low", "medium", or "high"
# - low: Only block high-confidence threats
# - medium: Block medium and high confidence threats (recommended)
# - high: Block all detected threats including low confidence
sensitivity = "medium"
# Block requests when security threats are detected
block_on_detection = true
# Maximum violations before auto-blocking an IP
max_violations_before_block = 3
# Time window for counting violations (seconds)
violation_window_secs = 3600
# How long to block an IP after exceeding max violations (seconds)
block_duration_secs = 86400
# Immediately block IPs that send critical-level threats
auto_block_on_critical = true
# Custom patterns to detect (in addition to built-in patterns)
# custom_patterns = ["DROP TABLE", "eval("]

# ==============================
# Messenger Platform Selection
# ==============================
# Choose which messaging platform to use: "whatsapp", "signal", or "none"
# Only one messenger can be active at a time
messenger = "whatsapp"

# ==============================
# WhatsApp Business Integration
# ==============================
# Configure if messenger = "whatsapp"
[whatsapp]
# Meta Graph API access token
# access_token = "your-access-token"
# Phone number ID from WhatsApp Business
# phone_number_id = "your-phone-number-id"
# App secret for webhook signature verification
# app_secret = "your-app-secret"
# Verify token for webhook setup
# verify_token = "your-verify-token"
# Whether webhook signature verification is required
signature_required = true
# Meta Graph API version
api_version = "v18.0"
# Phone numbers allowed to send messages (empty = allow all)
# whitelist = ["+1234567890", "+0987654321"]

# ==============================
# Signal Integration (via signal-cli)
# ==============================
# Configure if messenger = "signal"
# Requires signal-cli running in JSON-RPC daemon mode
[signal]
# Your phone number registered with Signal (E.164 format)
# phone_number = "+1234567890"
# Path to signal-cli JSON-RPC socket
socket_path = "/var/run/signal-cli/socket"
# Path to signal-cli data directory (optional)
# data_path = "/var/lib/signal-cli"
# Connection timeout in milliseconds
timeout_ms = 30000
# Phone numbers allowed to send messages (empty = allow all)
# whitelist = ["+1234567890", "+0987654321"]

# ==============================
# Speech Processing (Voice Messages)
# ==============================
# Enable voice message support (STT/TTS)
#
# Cloud provider (OpenAI):
# - Works on all platforms
# - Requires API key
#
# Local provider (whisper.cpp + Piper):
# - Raspberry Pi: Models in /usr/local/share/{whisper,piper}/
# - macOS: Models in ~/Library/Application Support/{whisper,piper}/
# - Install whisper.cpp: brew install whisper-cpp (Mac) or build from source (Pi)
# - Install Piper: Download from https://github.com/rhasspy/piper/releases
#
# [speech]
# Speech provider: "openai" (cloud) or "local" (whisper.cpp + Piper)
# provider = "openai"
# OpenAI API key for Whisper (STT) and TTS
# openai_api_key = "sk-..."
# OpenAI API base URL (for custom endpoints)
# openai_base_url = "https://api.openai.com/v1"
# Speech-to-text model (OpenAI Whisper)
# stt_model = "whisper-1"
# Text-to-speech model
# tts_model = "tts-1"
# Default TTS voice: alloy, echo, fable, onyx, nova, shimmer
# default_voice = "nova"
# Output audio format: opus, ogg, mp3, wav
# output_format = "opus"
# Request timeout in milliseconds
# timeout_ms = 60000
# Maximum audio duration in milliseconds (25 min for Whisper)
# max_audio_duration_ms = 1500000
# Response format preference: mirror, text, voice
# response_format = "mirror"
# TTS speaking speed (0.25 to 4.0)
# speed = 1.0

# ==============================
# Memory/Knowledge Storage
# ==============================
# Persistent AI memory for RAG-based context retrieval.
# Stores interactions, facts, preferences, and corrections.
# Uses embeddings for semantic similarity search.
#
# [memory]
# Enable memory storage (default: true)
# enabled = true
# Enable RAG context retrieval (default: true)
# enable_rag = true
# Enable automatic learning from interactions (default: true)
# enable_learning = true
# Number of memories to retrieve for RAG context (default: 5)
# rag_limit = 5
# Minimum similarity threshold for RAG retrieval (0.0-1.0, default: 0.5)
# rag_threshold = 0.5
# Similarity threshold for memory deduplication (0.0-1.0, default: 0.85)
# merge_threshold = 0.85
# Minimum importance score to keep memories (default: 0.1)
# min_importance = 0.1
# Decay factor for memory importance over time (default: 0.95)
# decay_factor = 0.95
# Enable content encryption (default: true)
# enable_encryption = true
# Path to encryption key file (generated if not exists)
# encryption_key_path = "memory_encryption.key"
#
# [memory.embedding]
# Embedding model name (default: nomic-embed-text)
# model = "nomic-embed-text"
# Embedding dimension (default: 384 for nomic-embed-text)
# dimension = 384
# Request timeout in milliseconds (default: 30000)
# timeout_ms = 30000

# =====================
# Database Settings
# =====================
[database]
# Path to the SQLite database file
path = "pisovereign.db"
# Maximum number of connections in the pool
max_connections = 5
# Run migrations on startup
run_migrations = true

# =====================
# Cache Settings
# =====================
[cache]
# Enable caching (disable for debugging)
enabled = true
# Short TTL in seconds (for frequently changing data)
ttl_short_secs = 300  # 5 minutes
# Medium TTL in seconds (for moderately stable data)
ttl_medium_secs = 3600  # 1 hour
# Long TTL in seconds (for stable data)
ttl_long_secs = 86400  # 24 hours
# TTL for dynamic LLM responses (high temperature)
ttl_llm_dynamic_secs = 3600  # 1 hour
# TTL for stable LLM responses (low temperature/factual)
ttl_llm_stable_secs = 86400  # 24 hours
# Maximum entries in L1 (in-memory) cache
l1_max_entries = 10000

# ==============================
# Telemetry / OpenTelemetry
# ==============================
[telemetry]
# Enable OpenTelemetry export
enabled = false
# OTLP endpoint URL (e.g., for Tempo/Jaeger)
otlp_endpoint = "http://localhost:4317"
# Sampling ratio (0.0 to 1.0, where 1.0 = sample all traces)
sample_ratio = 1.0
# Service name for traces
# service_name = "pisovereign"
# Log level filter (e.g., "info", "debug", "pisovereign=debug,tower_http=info")
# log_filter = "pisovereign=info,tower_http=info"
# Batch export timeout in seconds
# export_timeout_secs = 30
# Maximum batch size for trace export
# max_batch_size = 512
# Graceful fallback to console-only logging if OTLP collector is unavailable.
# When true (default), the application starts with console logging if the collector
# cannot be reached. Set to false to require a working collector in production.
# graceful_fallback = true

# ==============================
# Degraded Mode / Resilience
# ==============================
[degraded_mode]
# Enable degraded mode fallback when backend is unavailable
enabled = true
# Message to return when service is unavailable
unavailable_message = "I'm currently experiencing technical difficulties. Please try again in a moment."
# Cooldown before retrying primary backend (seconds)
retry_cooldown_secs = 30
# Number of failures before entering degraded mode
failure_threshold = 3
# Number of successes required to exit degraded mode
success_threshold = 2

# ==============================
# Retry Configuration (Exponential Backoff)
# ==============================
[retry]
# Initial delay before first retry in milliseconds
initial_delay_ms = 100
# Maximum delay between retries in milliseconds
max_delay_ms = 10000
# Multiplier for exponential backoff (delay = initial * multiplier^attempt)
multiplier = 2.0
# Maximum number of retry attempts
max_retries = 3

# ==============================
# Health Check Configuration
# ==============================
[health]
# Global timeout for all health checks in seconds
global_timeout_secs = 5
# Service-specific timeout overrides (uncomment to customize):
# inference_timeout_secs = 10
# email_timeout_secs = 5
# calendar_timeout_secs = 5
# weather_timeout_secs = 5

# ==============================
# Weather Integration (Open-Meteo)
# ==============================
# [weather]
# Open-Meteo API base URL
# base_url = "https://api.open-meteo.com/v1"
# Connection timeout in seconds
# timeout_secs = 30
# Number of forecast days (1-16)
# forecast_days = 7
# Cache TTL in minutes
# cache_ttl_minutes = 30
# Default location for weather (used when user profile has no location)
# Inline table format: { latitude = 52.52, longitude = 13.405 }
# default_location = { latitude = 52.52, longitude = 13.405 }  # Berlin

# ==============================
# Web Search Integration (Brave/DuckDuckGo)
# ==============================
# Web search allows the LLM to search the internet for real-time information.
# Brave Search is the primary provider (requires API key), with DuckDuckGo as fallback.
# Get your Brave API key at: https://brave.com/search/api/
#
# [websearch]
# Brave Search API key (required for Brave, optional if using DuckDuckGo only)
# api_key = "BSA..."
# Maximum number of search results to return (1-10)
# max_results = 5
# Connection timeout in seconds
# timeout_secs = 30
# Enable DuckDuckGo fallback when Brave fails or returns no results
# fallback_enabled = true
# Safe search level: "off", "moderate", or "strict"
# safe_search = "moderate"
# Country code for search results (e.g., "DE", "US", "GB")
# country = "DE"
# Language code for search results (e.g., "de", "en", "fr")
# language = "de"
# Rate limit: maximum requests per minute (0 = unlimited)
# rate_limit_rpm = 60
# Cache TTL in minutes for search results
# cache_ttl_minutes = 30

# ==============================
# CalDAV Calendar Integration
# ==============================
# [caldav]
# CalDAV server URL (e.g., Baikal, Radicale, Nextcloud)
# server_url = "https://cal.example.com"
# When using Ba√Økal via Docker (setup --baikal):
# server_url = "http://baikal:80/dav.php"
# Username for authentication
# username = "your-username"
# Password for authentication
# password = "your-password"
# Default calendar path (optional)
# calendar_path = "/calendars/user/default"
# Verify TLS certificates
# verify_certs = true
# Connection timeout in seconds
# timeout_secs = 30

# ==============================
# Proton Mail Integration
# ==============================
# [proton]
# IMAP server host (Proton Bridge)
# imap_host = "127.0.0.1"
# IMAP server port (default: 1143 for STARTTLS)
# imap_port = 1143
# SMTP server host (Proton Bridge)
# smtp_host = "127.0.0.1"
# SMTP server port (default: 1025 for STARTTLS)
# smtp_port = 1025
# Email address (Bridge account email)
# email = "user@proton.me"
# Bridge password (from Bridge UI, NOT Proton account password)
# password = "bridge-password"
# TLS configuration
# [proton.tls]
# verify_certificates = false  # Omit to verify (secure default), set false for self-signed Bridge certs
# min_tls_version = "1.2"
# ca_cert_path = "/path/to/ca.pem"  # Optional custom CA certificate

# ==============================
# Model Selector (Dynamic Routing)
# ==============================
# [model_selector]
# Model for simple/fast tasks
# small_model = "qwen2.5-1.5b-instruct"
# Model for complex/quality tasks
# large_model = "qwen2.5-7b-instruct"
# Word count threshold to trigger large model
# complexity_word_threshold = 100
# Maximum prompt length (chars) for small model
# small_model_max_prompt_chars = 500
# Keywords that trigger large model usage
# complexity_keywords = ["analyze", "explain", "compare", "summarize", "code", "implement", "debug", "refactor", "translate", "research"]
